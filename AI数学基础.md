# 1. AI数学基础
<!-- TOC -->

- [1. AI数学基础](#1-ai数学基础)
    - [1.1. 线性代数](#11-线性代数)
        - [1.1.1. 理论](#111-理论)
        - [1.1.2. 实验](#112-实验)
    - [1.2. 概率论](#12-概率论)
        - [1.2.1. 基础理论](#121-基础理论)
        - [1.2.2. 实验](#122-实验)
    - [1.3. 最优化问题](#13-最优化问题)
        - [1.3.1. 理论](#131-理论)
        - [1.3.2. 实验](#132-实验)
<!-- /TOC -->
## 1.1. 线性代数

### 1.1.1. 理论
* 向量
* 标量
* 矩阵
    * 特征向量与特征值
        * 设A是数域K上的n阶方阵，如果K^n中有非零列向量α使得Aα=λα，且λ属于K，则称λ是A的一个特征值，称α是A的属于特征值λ的一个特征向量
    * 特征分解
        方阵有n个线性无关的特征向量α...相对应的特征值为λ
    * 线性空间，特征值越大，则矩阵在对应的特征向量上的方差越大，信息量愈多
    * 最优化问题中，矩阵特征值的大小与函数的变化快慢有关系，在最大特征值所对应的特征方向上函数值变化最大，也就是该方向上的导数越大
    * 特异值分解：矩阵分解为奇异向量和奇异值(将一个矩阵分解为三个矩阵的乘积)，应用：PCA、数据压缩、特征提取、数字水印和潜在语义分析
* 张量(Tensor):一个多维数组
    * 零阶张量，标量
    * 一阶张量：向量
    * 二阶张量：矩阵
    * ...

### 1.1.2. 实验

1. reshape：改变矩阵的维度
2. .T：转置
3. 矩阵运算
4. .inv：逆矩阵
5. np.linalg.det(E)：计算矩阵对应行列式的值
6. 奇异值分解
```python
U,s,Vh = np.linalg.svd(V)# 分别对应左奇异矩阵、奇异值矩阵、右奇异值矩阵
```
可以应用于图片压缩
```python
import numpy as np
from pylab import *
import matplotlib.pyplot as plt

# 读取图像，保存为灰度图像
img = imread(path)[:,:,0]
plt.savefig(newPath)
plt.gray()
# 画出灰度图
plt.figure(1)
plt.imshow(img)

# 读取并打印图像长度
m,n = img.shape
print(np.shape(img))

# 对图像进行奇异值分解
U,sigma,V = np.linalg.svd(img)
# 打印奇异值大小
print(np.shape(sigma))

# 奇异值整理为对角矩阵
sigma = resize(sigma,[m,1])*eye(m,n)
# 取k个奇异值及其奇异向量用于压缩图像
k = 100
# 用前k个奇异值及其起义先来个你构造新图像
img1 = np.dot(U[:,0:k],np.dot(sigma[0:k,0,k],V[0:k,:]))
plt.figure(2)

# 效果
plt.img(img1)
plt.show()
```
求解线性方程组
```python
import numpy as np

# 输入整理后等式左边的系数
a = np.array([...],[...],..)
# 输入等式右边的值
b = np.array([...])
x = solve(a,b)

# 输出结果
print(x)
```

## 1.2. 概率论

### 1.2.1. 基础理论
* 随机变量及其分布
    * 随机试验：
        * 相同条件
        * 结果不止一个，但是直到所有结果
        * 不知道会出现那个结果
    * 样本空间：由样本点组成
    * 随机事件，某些样本点组成的集合
    * 随机变量：本质是一个函数
        * 离散随机变量：所有可能取到的点是有限个或无法列举多个
        * 连续随机变量：全部可能取到的值有无限多个，或数值无法列举
    * 分布律：概率质量函数，pk>=0,k=1,2,3,...。
    * 特殊分布
        * 伯努利分布（两点分布）
            * 二分类，贝叶斯进行文本分类
            * 防止模型过拟合
        * 二项分布：
            * NLP中如计算含有某个特殊字符的所有句子占所有句子的百分比->确定一个动词在语言中常被用于及物动词还是非及物动词
            8 Dropout方法中：对于某一层的n个神经元在每个训练步骤中可以被看作是n个两点分布的集合
        * 泊松分布：
            * 二项分布中n极大p很小的近似计算
            * 单位i时间欸随机时间发生的次数，如某一段时间内某一客服电话收到的服务的请求的次数，汽车站台的侯客人数、机器故障自然灾害的发生次数、DNA序列的变异数
            * 图像处理中图像会因为观点显示仪器测量造成的不确定性而出现的服从泊松分布的泊松噪声
    * 分布函数：累积分布函数
        * 正态分布：高斯分布最常见：
            * 增加高斯噪声图像增强
* 随机向量及其分布
    * 随机向量：多个随机变量放在一起组成的向量，可以称为多维随机向量
    * 联合概率密度
* 贝叶斯公式
    * 后验概率：已知某一件事件发生的情况下另一个有关事情发生的概率
        * 应用：中文分词、统计机器翻译、深度学习网络
* 数学期望

* 方差：衡量随机变量或一组数据离散程度的度量，随机变量和器数学期望之间的偏离程度（一维
    * 协方差：两个随机变量线性相关心的强度
    * 相关系数：线性相关系数

### 1.2.2. 实验
```python
import numpy as np
import scipy as sp
```
1. `np.mean()`输入数据，第二个参数1：行求均值；2：列求均值
2. `np.var()`方差：输入与`mean()`一样
3. `np.std()`标准差
4. `np.cov()`协方差
5. `np.corrcoef()`两个矩阵之间元素的相关系数
```python
 from scipy.stats import binom,norm,beta,expon
 ```
6. `binom.rvs(n=,p=,size=)`二项分布
7. `np.random.poisson(lam=,size=)`泊松分布
8. `norm.pdf(x,mu,sigma)`正态分布


## 1.3. 最优化问题

### 1.3.1. 理论
得到目标函数的最大或最小化，也称为**代价函数、损失函数、误差函数**
* 无约束最优化问题：minf(x)
    * 解析法
    * 直接发
    * 梯度下降法：极值点出现在驻点
* 约束优化：优化我呢提芬治，某些集合中的最大值或者最小值，集合中的点称为可行点
    * 等式约束：比如路径一定要经过某个点
    * 不等式约束：比如投资金额要小于某个值

### 1.3.2. 实验
1. 最小二乘法->多项式拟合：`leastsq(func=,x0=,args=)`残差函数、随机初始化多项式参数、真是函数
2. 梯度下降函数

## 1.4. 错题概念
1. 相关系数:相关系数是用以反映变量之间相关关系密切程度的统计指标   
    * 简单相关系数：又叫相关系数或线性相关系数，一般用字母r表示，用来度量两个变量间的线性关系
    * 复相关系数：又叫多重相关系数。复相关是指因变量与多个自变量之间的相关关系。例如，某种商品的季节性需求量与其价格水平、职工收入水平等现象之间呈现复相关关系
    * 典型相关系数：是先对原来各组变量进行主成分分析，得到新的线性关系的综合指标，再通过综合指标之间的线性相关系数来研究原各组变量间相关关系。
2. 朴素贝叶斯:朴素贝叶斯必须假定所有特征是无关联的
3. PCA
    * 主成分分析（Principal Component Analysis，PCA）， 是一种统计方法。通过**正交变换**将一组可能存在相关性的变量转换为**一组线性~~不~~相关的变量**，转换后的这组变量叫主成分。